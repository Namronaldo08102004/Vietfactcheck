import sys
import os
import streamlit as st
import json
import random
import torch

# --- X·ª¨ L√ù PATH H·ªÜ TH·ªêNG ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.settings import settings
from src.components.vectorDB import VietnameseVectorDB
from src.components.reranker import VietnameseReranker
from src.modules.document_retrieval import DocumentRetrievalModule
from src.modules.evidence_selection import EvidenceSelectionModule
from src.modules.claim_verification import ClaimVerificationModule
from src.modules.claim_extraction import BERTSumClaimExtractor

import src.components.presumm.model as _models
sys.modules["models"] = _models

# --- C·∫§U H√åNH GIAO DI·ªÜN ---
st.set_page_config(page_title="VietFactCheck System", layout="wide", initial_sidebar_state="expanded")

st.markdown("""
    <style>
    .stButton>button { 
        width: 100%; 
        border-radius: 8px; 
        border: 1px solid #ff4b4b; 
        font-weight: bold;
        transition: 0.3s;
        margin-bottom: 10px;
    }
    .stButton>button:hover {
        background-color: #ff4b4b;
        color: white;
    }
    .highlight { 
        background-color: #fff2cc; 
        border: 1px solid #ffd966; 
        padding: 2px; 
        border-radius: 4px; 
        color: #333;
        font-weight: 500;
    }
    .claim-step {
        padding: 10px;
        border-radius: 5px;
        margin: 5px 0;
        border-left: 5px solid #ff4b4b;
        background-color: #f9f9f9;
    }
    </style>
""", unsafe_allow_html=True)

st.title("üõ°Ô∏è H·ªá th·ªëng X√°c th·ª±c Th√¥ng tin Ti·∫øng Vi·ªát")

TOPIC_ICONS = {
    'khoa h·ªçc': 'üß™', 'vƒÉn ho√°': 'üé®', 'vƒÉn h√≥a': 'üé®', 'qu√¢n s·ª±': 'üõ°Ô∏è', 'khoa gi√°o': 'üìö',
    'kinh doanh': 'üíº', 'ch√≠nh tr·ªã': 'üèõÔ∏è', 'th·∫ø gi·ªõi': 'üåç', 'th·ªùi s·ª±': 'üóûÔ∏è', 's·ª©c kho·∫ª': 'üè•',
    's·ª©c kh·ªèe': 'üè•', 'ƒë·ªùi s·ªëng': 'üå±', 'gi·∫£i tr√≠': 'üé¨', 'hoa h·∫≠u': 'üëë', 'kinh t·∫ø': 'üìà',
    'an ninh tr·∫≠t t·ª±': 'üëÆ', 'ph√°p lu·∫≠t': '‚öñÔ∏è', 'th·ªÉ thao': '‚öΩ', 'du l·ªãch': '‚úàÔ∏è', 'ƒë·ªãa ph∆∞∆°ng': 'üìç',
    'gi·ªõi tr·∫ª': 'üåà', 'b·∫•t ƒë·ªông s·∫£n': 'üè†', 'gi√°o d·ª•c': 'üéì', 's·ªë h√≥a': 'üî¢', 'ng∆∞·ªùi l√≠nh': 'üéñÔ∏è',
    'nh·ªãp s·ªëng ph∆∞∆°ng nam': 'üèôÔ∏è', 'x√£ h·ªôi': 'üë•', 'qu·ªëc t·∫ø': 'üåê', 'y t·∫ø': 'üíâ', 'ƒë·ªãa ·ªëc': 'üèóÔ∏è',
    'ƒë√¥ th·ªã': 'üåÜ', 'c√¥ng ngh·ªá': 'üíª', 'khoa h·ªçc c√¥ng ngh·ªá': 'üöÄ', 'nh√† ƒë·∫•t': 'üè°', 
    'gi√°o d·ª•c - h∆∞·ªõng nghi·ªáp': 'üìñ', 'b·∫°n ƒë·ªçc l√†m b√°o': '‚úçÔ∏è', 'vƒÉn h√≥a - x√£ h·ªôi': 'üé≠'
}

# --- H√ÄM KH·ªûI T·∫†O H·ªÜ TH·ªêNG ---
@st.cache_data
def load_recommendations():
    path = settings.DATA_PATHS.get("train")
    recs = {}
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            random.shuffle(data)
            for item in data:
                topic = item.get("Topic", "kh√°c").strip().lower()
                if topic not in recs: recs[topic] = item.get("Statement", "")
    return recs

@st.cache_data
def load_news_recommendations():
    path = settings.EXTRACTION_DATA_PATHS.get("train")
    recs = {}
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            random.shuffle(data)
            for item in data:
                topic = item.get("topic", "kh√°c").strip().lower()
                if topic not in recs: recs[topic] = item.get("fake_context", "")
    return recs

@st.cache_resource
def init_core_system():
    db = VietnameseVectorDB("master_db", settings.STORAGE_DIR, 
                            settings.EMBEDDING_MODEL, settings.TRUNCATION_DIM)
    ret_mod = DocumentRetrievalModule(db)
    if not db.load():
        ret_mod.build_system(list(settings.DATA_PATHS.values()))
    
    url_map = {}
    for p in settings.DATA_PATHS.values():
        if os.path.exists(p):
            with open(p, "r", encoding="utf-8") as f:
                for item in json.load(f): url_map[item['Url']] = item['Context']
    
    extractor = BERTSumClaimExtractor(
        model_path = getattr(settings, "EXTRACTOR_MODEL_PATH", "bertext_cnndm_transformer.pt"),
        visible_gpus = "-1" if torch.cuda.is_available() == False else "0"
    )
                
    return ret_mod, EvidenceSelectionModule(db), url_map, VietnameseReranker(), extractor

ret_mod, ev_mod, url_to_context, reranker, extractor = init_core_system()
recs_dict = load_recommendations()
news_dict = load_news_recommendations()

if "main_input" not in st.session_state: st.session_state["main_input"] = ""
if "rec_mode" not in st.session_state: st.session_state["rec_mode"] = "claim" 

# --- SIDEBAR: ƒêI·ªÄU KHI·ªÇN ---
st.sidebar.title("üéÆ Control Panel")
target_stage = st.sidebar.selectbox("Giai ƒëo·∫°n d·ª´ng x·ª≠ l√Ω:", 
                                    ["Document Retrieval", "Evidence Selection", "Claim Verification"])

st.sidebar.subheader("üé® Giao di·ªán g·ª£i √Ω")
grid_cols = st.sidebar.slider("S·ªë c·ªôt hi·ªÉn th·ªã Topic:", 2, 8, 6)

# 1. Document Retrieval Settings
with st.sidebar.expander("1. Document Retrieval Settings", expanded=True):
    dr_w_emb = st.slider("Embedding Weight", 0.0, 1.0, 0.4, key="dr_emb")
    dr_w_bm25 = st.slider("BM25 Weight", 0.0, 1.0, 0.3, key="dr_bm25")
    dr_w_tfidf = 1.0 - dr_w_emb - dr_w_bm25
    st.slider("TF-IDF Weight (C·ªë ƒë·ªãnh)", 0.0, 1.0, max(0.0, dr_w_tfidf), disabled=True)
    
    # --- Y√äU C·∫¶U 2: C·∫£nh b√°o UI cho DR ---
    if dr_w_emb + dr_w_bm25 > 1.0:
        st.error("‚ö†Ô∏è T·ªïng tr·ªçng s·ªë v∆∞·ª£t qu√° 1.0!")

    dr_use_rerank = st.toggle("S·ª≠ d·ª•ng Reranker cho Document?")
    dr_top_k = st.number_input("Top K URLs", 1, 10, 3 if dr_use_rerank else 1)

MODEL_MAPPING = {
    "XLM-RoBERTa-base": "Vifactcheck-xlm-roberta-base",
    "XLM-RoBERTa-large": "Vifactcheck-xlm-roberta-large",
    "ViBERT": "Vifactcheck-ViBERT",
    "mBERT": "Vifactcheck-mBERT",
    "PhoBERT-base": "Vifactcheck-phoBERT-base",
    "PhoBERT-large": "Vifactcheck-phoBERT-large"
}

v_mode = "Selected Evidences"
selected_hf_model = ""

if target_stage == "Claim Verification":
    with st.sidebar.expander("3. Claim Verification Settings", expanded=True):
        v_mode = st.radio("X√°c th·ª±c d·ª±a tr√™n:", ["Full Context", "Selected Evidences"])
        display_model_name = st.selectbox("Ch·ªçn Model PLM:", list(MODEL_MAPPING.keys()))
        base_name = MODEL_MAPPING[display_model_name]
        suffix = "-gold-evidence" if v_mode == "Selected Evidences" else ""
        selected_hf_model = f"Namronaldo2004/{base_name}{suffix}"

show_ev = (target_stage == "Evidence Selection") or (target_stage == "Claim Verification" and v_mode == "Selected Evidences")
if show_ev:
    with st.sidebar.expander("2. Evidence Selection Settings", expanded=True):
        ev_w_emb = st.slider("Evid. Embedding Weight", 0.0, 1.0, 0.6, key="ev_emb")
        ev_w_bm25 = st.slider("Evid. BM25 Weight", 0.0, 1.0, 0.2, key="ev_bm25")
        
        # --- Y√äU C·∫¶U 1: B·ªï sung hi·ªÉn th·ªã TF-IDF Weight cho Evidence ---
        ev_w_tfidf = 1.0 - ev_w_emb - ev_w_bm25
        st.slider("Evid. TF-IDF Weight (C·ªë ƒë·ªãnh)", 0.0, 1.0, max(0.0, ev_w_tfidf), disabled=True)
        
        # --- Y√äU C·∫¶U 2: C·∫£nh b√°o UI cho Evidence ---
        if ev_w_emb + ev_w_bm25 > 1.0:
            st.error("‚ö†Ô∏è T·ªïng tr·ªçng s·ªë v∆∞·ª£t qu√° 1.0!")

        ev_use_rerank = st.toggle("S·ª≠ d·ª•ng Reranker cho Evidence?", value=True)
        if ev_use_rerank:
            ev_top_k_input = st.number_input("S·ªë l∆∞·ª£ng b·∫±ng ch·ª©ng tr∆∞·ªõc Rerank:", 3, 20, 10)
            t1 = st.slider("Confidence Threshold (T1)", 0.6, 1.0, 0.75)
            t2 = st.slider("Gap Threshold (T2)", 0.0, 0.15, 0.05)
        else:
            ev_top_k_input = st.number_input("S·ªë l∆∞·ª£ng b·∫±ng ch·ª©ng (Top K):", 1, 10, 3)

# --- KHU V·ª∞C G·ª¢I √ù ---
col_title, col_nav = st.columns([0.8, 0.2])
with col_title:
    if st.session_state["rec_mode"] == "claim":
        st.subheader("üí° G·ª£i √Ω Claim theo ch·ªß ƒë·ªÅ")
        current_data = recs_dict
    else:
        st.subheader("üì∞ G·ª£i √Ω b·∫£n tin th·ªùi s·ª± theo ch·ªß ƒë·ªÅ")
        current_data = news_dict

with col_nav:
    if st.session_state["rec_mode"] == "claim":
        if st.button("Ti·∫øp theo ‚û°Ô∏è"):
            st.session_state["rec_mode"] = "news"
            st.rerun()
    else:
        if st.button("‚¨ÖÔ∏è Quay l·∫°i"):
            st.session_state["rec_mode"] = "claim"
            st.rerun()

topic_list = list(current_data.keys())
for i in range(0, len(topic_list), grid_cols):
    cols = st.columns(grid_cols)
    for j in range(grid_cols):
        if i + j < len(topic_list):
            topic = topic_list[i + j]
            icon = TOPIC_ICONS.get(topic, 'üìù')
            if cols[j].button(f"{icon} {topic.capitalize()}", key=f"btn_{topic}_{st.session_state['rec_mode']}"):
                st.session_state["main_input"] = current_data[topic]
                st.rerun()

st.divider()

# --- GIAO DI·ªÜN CH√çNH ---
claim_text = st.text_area("Nh·∫≠p n·ªôi dung c·∫ßn ki·ªÉm ch·ª©ng (Claim):", key="main_input", height=150)
use_extraction = st.checkbox("Chia nh·ªè n·ªôi dung ƒë·∫ßu v√†o th√†nh c√°c claim ri√™ng bi·ªát ƒë·ªÉ ki·ªÉm ch·ª©ng", value=False)

if st.button("üöÄ B·∫Øt ƒë·∫ßu th·ª±c hi·ªán x·ª≠ l√Ω", type="primary"):
    # --- Y√äU C·∫¶U 2: Ki·ªÉm tra tr·ªçng s·ªë tr∆∞·ªõc khi x·ª≠ l√Ω ---
    if dr_w_emb + dr_w_bm25 > 1.0:
        st.error("‚ùå Kh√¥ng th·ªÉ th·ª±c hi·ªán: T·ªïng tr·ªçng s·ªë Document Retrieval v∆∞·ª£t qu√° 1.0. Vui l√≤ng ƒëi·ªÅu ch·ªânh l·∫°i ·ªü thanh b√™n!")
        st.stop()
    
    if show_ev and (ev_w_emb + ev_w_bm25 > 1.0):
        st.error("‚ùå Kh√¥ng th·ªÉ th·ª±c hi·ªán: T·ªïng tr·ªçng s·ªë Evidence Selection v∆∞·ª£t qu√° 1.0. Vui l√≤ng ƒëi·ªÅu ch·ªânh l·∫°i ·ªü thanh b√™n!")
        st.stop()

    if not claim_text.strip():
        st.warning("Vui l√≤ng nh·∫≠p n·ªôi dung!")
        st.stop()

    # X·ª≠ l√Ω danh s√°ch Claim
    claims_to_process = []
    if use_extraction:
        with st.spinner("‚úÇÔ∏è ƒêang ph√¢n t√°ch n·ªôi dung..."):
            claims_to_process = extractor.extract(claim_text)
            if not claims_to_process:
                st.error("Kh√¥ng th·ªÉ t√°ch ƒë∆∞·ª£c claim n√†o. S·ª≠ d·ª•ng n·ªôi dung g·ªëc.")
                claims_to_process = [claim_text]
            else:
                st.info(f"‚úÖ ƒê√£ t√¨m th·∫•y **{len(claims_to_process)}** claim c·∫ßn x√°c th·ª±c.")
    else:
        claims_to_process = [claim_text]

    claim_tabs = st.tabs([f"Claim {i+1}" for i in range(len(claims_to_process))])

    for idx, (current_claim, tab) in enumerate(zip(claims_to_process, claim_tabs)):
        with tab:
            st.markdown(f"**N·ªôi dung ki·ªÉm ch·ª©ng:** *{current_claim}*")
            
            # --- B∆Ø·ªöC 1: DOCUMENT RETRIEVAL ---
            with st.status(f"üîç [C{idx+1}] ƒêang truy xu·∫•t b√†i b√°o...") as s:
                dr_weights = (dr_w_bm25, 1.0 - dr_w_emb - dr_w_bm25, dr_w_emb)
                urls = ret_mod.get_top_k_url(current_claim, top_k=dr_top_k, weights=dr_weights)
                
                if dr_use_rerank:
                    class Item:
                        def __init__(self, url, content): 
                            self.url, self.page_content = url, content
                    cands = [Item(u, url_to_context[u]) for u in urls]
                    best_url = reranker.rerank(current_claim, cands)[0]['document'].url
                else:
                    best_url = urls[0]
                s.update(label="‚úÖ ƒê√£ t√¨m th·∫•y ngu·ªìn!", state="complete")

            st.markdown(f"**Ngu·ªìn:** [{best_url}]({best_url})")
            full_text = url_to_context.get(best_url, "")

            if target_stage == "Document Retrieval":
                st.write(full_text)
                continue

            # --- B∆Ø·ªöC 2: EVIDENCE SELECTION ---
            selected_evidences = []
            if show_ev:
                with st.status(f"üìç [C{idx+1}] ƒêang tr√≠ch xu·∫•t b·∫±ng ch·ª©ng...") as s:
                    ev_weights = (ev_w_bm25, 1.0 - ev_w_emb - ev_w_bm25, ev_w_emb)
                    cands = ev_mod.select_top_k_evidence(current_claim, best_url, top_k=ev_top_k_input, weights=ev_weights)
                    
                    if ev_use_rerank:
                        reranked_ev = reranker.rerank(current_claim, cands)
                        high_score = [res for res in reranked_ev if res['rerank_score'] > t1]
                        if high_score:
                            selected_evidences = [res['document'] for res in high_score]
                        else:
                            selected_evidences = [reranked_ev[0]['document']]
                            for i in range(1, len(reranked_ev)):
                                if (reranked_ev[i-1]['rerank_score'] - reranked_ev[i]['rerank_score']) < t2:
                                    selected_evidences.append(reranked_ev[i]['document'])
                                else: break
                    else:
                        selected_evidences = cands[:ev_top_k_input]
                    s.update(label=f"‚úÖ {len(selected_evidences)} b·∫±ng ch·ª©ng!", state="complete")

                highlighted_html = full_text
                for ev in selected_evidences:
                    snippet = ev.page_content.strip()
                    highlighted_html = highlighted_html.replace(snippet, f'<span class="highlight">{snippet}</span>')
                st.markdown(f"<div style='text-align: justify;'>{highlighted_html}</div>", unsafe_allow_html=True)
            else:
                st.write(full_text)

            if target_stage == "Evidence Selection":
                continue

            # --- B∆Ø·ªöC 3: CLAIM VERIFICATION ---
            if target_stage == "Claim Verification":
                with st.spinner(f"‚öñÔ∏è ƒêang x√°c th·ª±c Claim {idx+1}..."):
                    verifier = ClaimVerificationModule(selected_hf_model)
                    result = verifier.verify_claim(
                        current_claim, 
                        full_context=full_text if v_mode == "Full Context" else None,
                        evidences=selected_evidences if v_mode == "Selected Evidences" else None
                    )
                    
                    st.divider()
                    label = result['label_name']
                    if label == "Supported":
                        st.success(f"‚úÖ **CH√çNH X√ÅC**")
                    elif label == "Refuted":
                        st.error(f"‚ùå **SAI S·ª∞ TH·∫¨T**")
                    else:
                        st.warning(f"‚ùì **KH√îNG ƒê·ª¶ TH√îNG TIN**")