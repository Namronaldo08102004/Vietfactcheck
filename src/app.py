import sys
import os
import streamlit as st
import json
import random

# --- Xá»¬ LÃ PATH Há»† THá»NG ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.settings import settings
from src.components.vectorDB import VietnameseVectorDB
from src.components.reranker import VietnameseReranker
from src.modules.document_retrieval import DocumentRetrievalModule
from src.modules.evidence_selection import EvidenceSelectionModule
from src.modules.claim_verification import ClaimVerificationModule
from src.modules.claim_extraction import BERTSumClaimExtractor

import src.components.presumm.model as _models
sys.modules["models"] = _models

# --- Cáº¤U HÃŒNH GIAO DIá»†N ---
st.set_page_config(page_title="VietFactCheck System", layout="wide", initial_sidebar_state="expanded")

st.markdown("""
    <style>
    .stButton>button { 
        width: 100%; 
        border-radius: 8px; 
        border: 1px solid #ff4b4b; 
        font-weight: bold;
        transition: 0.3s;
        margin-bottom: 10px;
    }
    .stButton>button:hover {
        background-color: #ff4b4b;
        color: white;
    }
    .highlight { 
        background-color: #fff2cc; 
        border: 1px solid #ffd966; 
        padding: 2px; 
        border-radius: 4px; 
        color: #333;
        font-weight: 500;
    }
    /* Style cho step indicator */
    .claim-step {
        padding: 10px;
        border-radius: 5px;
        margin: 5px 0;
        border-left: 5px solid #ff4b4b;
        background-color: #f9f9f9;
    }
    </style>
""", unsafe_allow_html=True)

st.title("ğŸ›¡ï¸ Há»‡ thá»‘ng XÃ¡c thá»±c ThÃ´ng tin Tiáº¿ng Viá»‡t")

TOPIC_ICONS = {
    'khoa há»c': 'ğŸ§ª', 'vÄƒn hoÃ¡': 'ğŸ¨', 'vÄƒn hÃ³a': 'ğŸ¨', 'quÃ¢n sá»±': 'ğŸ›¡ï¸', 'khoa giÃ¡o': 'ğŸ“š',
    'kinh doanh': 'ğŸ’¼', 'chÃ­nh trá»‹': 'ğŸ›ï¸', 'tháº¿ giá»›i': 'ğŸŒ', 'thá»i sá»±': 'ğŸ—ï¸', 'sá»©c khoáº»': 'ğŸ¥',
    'sá»©c khá»e': 'ğŸ¥', 'Ä‘á»i sá»‘ng': 'ğŸŒ±', 'giáº£i trÃ­': 'ğŸ¬', 'hoa háº­u': 'ğŸ‘‘', 'kinh táº¿': 'ğŸ“ˆ',
    'an ninh tráº­t tá»±': 'ğŸ‘®', 'phÃ¡p luáº­t': 'âš–ï¸', 'thá»ƒ thao': 'âš½', 'du lá»‹ch': 'âœˆï¸', 'Ä‘á»‹a phÆ°Æ¡ng': 'ğŸ“',
    'giá»›i tráº»': 'ğŸŒˆ', 'báº¥t Ä‘á»™ng sáº£n': 'ğŸ ', 'giÃ¡o dá»¥c': 'ğŸ“', 'sá»‘ hÃ³a': 'ğŸ”¢', 'ngÆ°á»i lÃ­nh': 'ğŸ–ï¸',
    'nhá»‹p sá»‘ng phÆ°Æ¡ng nam': 'ğŸ™ï¸', 'xÃ£ há»™i': 'ğŸ‘¥', 'quá»‘c táº¿': 'ğŸŒ', 'y táº¿': 'ğŸ’‰', 'Ä‘á»‹a á»‘c': 'ğŸ—ï¸',
    'Ä‘Ã´ thá»‹': 'ğŸŒ†', 'cÃ´ng nghá»‡': 'ğŸ’»', 'khoa há»c cÃ´ng nghá»‡': 'ğŸš€', 'nhÃ  Ä‘áº¥t': 'ğŸ¡', 
    'giÃ¡o dá»¥c - hÆ°á»›ng nghiá»‡p': 'ğŸ“–', 'báº¡n Ä‘á»c lÃ m bÃ¡o': 'âœï¸', 'vÄƒn hÃ³a - xÃ£ há»™i': 'ğŸ­'
}

# --- HÃ€M KHá»I Táº O Há»† THá»NG ---
@st.cache_data
def load_recommendations():
    """Láº¥y claim vÃ­ dá»¥"""
    path = settings.DATA_PATHS.get("train")
    recs = {}
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            random.shuffle(data)
            for item in data:
                topic = item.get("Topic", "khÃ¡c").strip().lower()
                if topic not in recs: recs[topic] = item.get("Statement", "")
    return recs

@st.cache_data
def load_news_recommendations():
    """Láº¥y báº£n tin thá»i sá»± vÃ­ dá»¥"""
    path = settings.EXTRACTION_DATA_PATHS.get("train")
    recs = {}
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            random.shuffle(data)
            for item in data:
                topic = item.get("topic", "khÃ¡c").strip().lower()
                if topic not in recs: recs[topic] = item.get("fake_context", "")
    return recs

@st.cache_resource
def init_core_system():
    db = VietnameseVectorDB("master_db", settings.STORAGE_DIR, 
                            settings.EMBEDDING_MODEL, settings.TRUNCATION_DIM)
    ret_mod = DocumentRetrievalModule(db)
    if not db.load():
        ret_mod.build_system(list(settings.DATA_PATHS.values()))
    
    url_map = {}
    for p in settings.DATA_PATHS.values():
        if os.path.exists(p):
            with open(p, "r", encoding="utf-8") as f:
                for item in json.load(f): url_map[item['Url']] = item['Context']
    
    # Khá»Ÿi táº¡o Extractor (Sá»­ dá»¥ng model path tá»« settings náº¿u cÃ³)
    extractor = BERTSumClaimExtractor(model_path = getattr(settings, "EXTRACTOR_MODEL_PATH", "bertext_cnndm_transformer.pt"))
                
    return ret_mod, EvidenceSelectionModule(db), url_map, VietnameseReranker(), extractor

ret_mod, ev_mod, url_to_context, reranker, extractor = init_core_system()
recs_dict = load_recommendations()
news_dict = load_news_recommendations()

# Quáº£n lÃ½ Session State
if "main_input" not in st.session_state: st.session_state["main_input"] = ""
if "rec_mode" not in st.session_state: st.session_state["rec_mode"] = "claim" # 'claim' or 'news'

# --- SIDEBAR: ÄIá»€U KHIá»‚N ---
st.sidebar.title("ğŸ® Control Panel")
target_stage = st.sidebar.selectbox("Giai Ä‘oáº¡n dá»«ng xá»­ lÃ½:", 
                                    ["Document Retrieval", "Evidence Selection", "Claim Verification"])

st.sidebar.subheader("ğŸ¨ Giao diá»‡n gá»£i Ã½")
grid_cols = st.sidebar.slider("Sá»‘ cá»™t hiá»ƒn thá»‹ Topic:", 2, 8, 6)

# 1. Document Retrieval Settings
with st.sidebar.expander("1. Document Retrieval Settings", expanded=True):
    dr_w_emb = st.slider("Embedding Weight", 0.0, 1.0, 0.4, key="dr_emb")
    dr_w_bm25 = st.slider("BM25 Weight", 0.0, 1.0, 0.3, key="dr_bm25")
    dr_w_tfidf = 1.0 - dr_w_emb - dr_w_bm25
    st.slider("TF-IDF Weight (Cá»‘ Ä‘á»‹nh)", 0.0, 1.0, max(0.0, dr_w_tfidf), disabled=True)
    dr_use_rerank = st.toggle("Sá»­ dá»¥ng Reranker cho Document?")
    dr_top_k = st.number_input("Top K URLs", 1, 10, 3 if dr_use_rerank else 1)

# Logic Model Mapping (YÃªu cáº§u 1)
MODEL_MAPPING = {
    "XLM-RoBERTa-base": "Vifactcheck-xlm-roberta-base",
    "XLM-RoBERTa-large": "Vifactcheck-xlm-roberta-large",
    "ViBERT": "Vifactcheck-ViBERT",
    "mBERT": "Vifactcheck-mBERT",
    "PhoBERT-base": "Vifactcheck-phoBERT-base",
    "PhoBERT-large": "Vifactcheck-phoBERT-large"
}

v_mode = "Selected Evidences"
selected_hf_model = ""

if target_stage == "Claim Verification":
    with st.sidebar.expander("3. Claim Verification Settings", expanded=True):
        v_mode = st.radio("XÃ¡c thá»±c dá»±a trÃªn:", ["Full Context", "Selected Evidences"])
        display_model_name = st.selectbox("Chá»n Model PLM:", list(MODEL_MAPPING.keys()))
        
        # Build tÃªn model HuggingFace dá»±a trÃªn mode
        base_name = MODEL_MAPPING[display_model_name]
        suffix = "-gold-evidence" if v_mode == "Selected Evidences" else ""
        selected_hf_model = f"Namronaldo2004/{base_name}{suffix}"

show_ev = (target_stage == "Evidence Selection") or (target_stage == "Claim Verification" and v_mode == "Selected Evidences")
if show_ev:
    with st.sidebar.expander("2. Evidence Selection Settings", expanded=True):
        ev_w_emb = st.slider("Evid. Embedding Weight", 0.0, 1.0, 0.6, key="ev_emb")
        ev_w_bm25 = st.slider("Evid. BM25 Weight", 0.0, 1.0, 0.2, key="ev_bm25")
        ev_use_rerank = st.toggle("Sá»­ dá»¥ng Reranker cho Evidence?", value=True)
        if ev_use_rerank:
            ev_top_k_input = st.number_input("Sá»‘ lÆ°á»£ng báº±ng chá»©ng trÆ°á»›c Rerank:", 3, 20, 10)
            t1 = st.slider("Confidence Threshold (T1)", 0.6, 1.0, 0.75)
            t2 = st.slider("Gap Threshold (T2)", 0.0, 0.15, 0.05)
        else:
            ev_top_k_input = st.number_input("Sá»‘ lÆ°á»£ng báº±ng chá»©ng (Top K):", 1, 10, 3)

# --- KHU Vá»°C Gá»¢I Ã (YÃªu cáº§u 2: Chuyá»ƒn Ä‘á»•i Claim/Báº£n tin) ---
col_title, col_nav = st.columns([0.8, 0.2])

with col_title:
    if st.session_state["rec_mode"] == "claim":
        st.subheader("ğŸ’¡ Gá»£i Ã½ Claim theo chá»§ Ä‘á»")
        current_data = recs_dict
    else:
        st.subheader("ğŸ“° Gá»£i Ã½ báº£n tin thá»i sá»± theo chá»§ Ä‘á»")
        current_data = news_dict

with col_nav:
    if st.session_state["rec_mode"] == "claim":
        if st.button("Tiáº¿p theo â¡ï¸"):
            st.session_state["rec_mode"] = "news"
            st.rerun()
    else:
        if st.button("â¬…ï¸ Quay láº¡i"):
            st.session_state["rec_mode"] = "claim"
            st.rerun()

topic_list = list(current_data.keys())
for i in range(0, len(topic_list), grid_cols):
    cols = st.columns(grid_cols)
    for j in range(grid_cols):
        if i + j < len(topic_list):
            topic = topic_list[i + j]
            icon = TOPIC_ICONS.get(topic, 'ğŸ“')
            if cols[j].button(f"{icon} {topic.capitalize()}", key=f"btn_{topic}_{st.session_state['rec_mode']}"):
                st.session_state["main_input"] = current_data[topic]
                st.rerun()

st.divider()

# --- GIAO DIá»†N CHÃNH ---
claim_text = st.text_area("Nháº­p ná»™i dung cáº§n kiá»ƒm chá»©ng (Claim):", key="main_input", height=150)

# Checkbox TÃ¡ch Claim (YÃªu cáº§u 2)
use_extraction = st.checkbox("Chia nhá» ná»™i dung Ä‘áº§u vÃ o thÃ nh cÃ¡c claim riÃªng biá»‡t Ä‘á»ƒ kiá»ƒm chá»©ng", value=False)

if st.button("ğŸš€ Báº¯t Ä‘áº§u thá»±c hiá»‡n xá»­ lÃ½", type="primary"):
    if not claim_text.strip():
        st.warning("Vui lÃ²ng nháº­p ná»™i dung!")
        st.stop()

    # Xá»­ lÃ½ danh sÃ¡ch Claim
    claims_to_process = []
    if use_extraction:
        with st.spinner("âœ‚ï¸ Äang phÃ¢n tÃ¡ch ná»™i dung..."):
            claims_to_process = extractor.extract(claim_text)
            if not claims_to_process:
                st.error("KhÃ´ng thá»ƒ tÃ¡ch Ä‘Æ°á»£c claim nÃ o. Sá»­ dá»¥ng ná»™i dung gá»‘c.")
                claims_to_process = [claim_text]
            else:
                st.info(f"âœ… ÄÃ£ tÃ¬m tháº¥y **{len(claims_to_process)}** claim cáº§n xÃ¡c thá»±c.")
    else:
        claims_to_process = [claim_text]

    # UI Step-by-Step cho tá»«ng Claim (Sá»­ dá»¥ng Tabs Ä‘á»ƒ ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ xem láº¡i)
    claim_tabs = st.tabs([f"Claim {i+1}" for i in range(len(claims_to_process))])

    for idx, (current_claim, tab) in enumerate(zip(claims_to_process, claim_tabs)):
        with tab:
            st.markdown(f"**Ná»™i dung kiá»ƒm chá»©ng:** *{current_claim}*")
            
            # --- BÆ¯á»šC 1: DOCUMENT RETRIEVAL ---
            with st.status(f"ğŸ” [C{idx+1}] Äang truy xuáº¥t bÃ i bÃ¡o...") as s:
                dr_weights = (dr_w_bm25, 1.0 - dr_w_emb - dr_w_bm25, dr_w_emb)
                urls = ret_mod.get_top_k_url(current_claim, top_k=dr_top_k, weights=dr_weights)
                
                if dr_use_rerank:
                    class Item:
                        def __init__(self, url, content): 
                            self.url, self.page_content = url, content
                    cands = [Item(u, url_to_context[u]) for u in urls]
                    best_url = reranker.rerank(current_claim, cands)[0]['document'].url
                else:
                    best_url = urls[0]
                s.update(label="âœ… ÄÃ£ tÃ¬m tháº¥y nguá»“n!", state="complete")

            st.markdown(f"**Nguá»“n:** [{best_url}]({best_url})")
            full_text = url_to_context.get(best_url, "")

            if target_stage == "Document Retrieval":
                st.write(full_text)
                continue

            # --- BÆ¯á»šC 2: EVIDENCE SELECTION ---
            selected_evidences = []
            if show_ev:
                with st.status(f"ğŸ“ [C{idx+1}] Äang trÃ­ch xuáº¥t báº±ng chá»©ng...") as s:
                    ev_weights = (ev_w_bm25, 1.0 - ev_w_emb - ev_w_bm25, ev_w_emb)
                    cands = ev_mod.select_top_k_evidence(current_claim, best_url, top_k=ev_top_k_input, weights=ev_weights)
                    
                    if ev_use_rerank:
                        reranked_ev = reranker.rerank(current_claim, cands)
                        high_score = [res for res in reranked_ev if res['rerank_score'] > t1]
                        if high_score:
                            selected_evidences = [res['document'] for res in high_score]
                        else:
                            selected_evidences = [reranked_ev[0]['document']]
                            for i in range(1, len(reranked_ev)):
                                if (reranked_ev[i-1]['rerank_score'] - reranked_ev[i]['rerank_score']) < t2:
                                    selected_evidences.append(reranked_ev[i]['document'])
                                else: break
                    else:
                        selected_evidences = cands[:ev_top_k_input]
                    s.update(label=f"âœ… {len(selected_evidences)} báº±ng chá»©ng!", state="complete")

                highlighted_html = full_text
                for ev in selected_evidences:
                    snippet = ev.page_content.strip()
                    highlighted_html = highlighted_html.replace(snippet, f'<span class="highlight">{snippet}</span>')
                st.markdown(f"<div style='text-align: justify;'>{highlighted_html}</div>", unsafe_allow_html=True)
            else:
                st.write(full_text)

            if target_stage == "Evidence Selection":
                continue

            # --- BÆ¯á»šC 3: CLAIM VERIFICATION ---
            if target_stage == "Claim Verification":
                with st.spinner(f"âš–ï¸ Äang xÃ¡c thá»±c Claim {idx+1}..."):
                    verifier = ClaimVerificationModule(selected_hf_model)
                    result = verifier.verify_claim(
                        current_claim, 
                        full_context=full_text if v_mode == "Full Context" else None,
                        evidences=selected_evidences if v_mode == "Selected Evidences" else None
                    )
                    
                    st.divider()
                    label = result['label_name']
                    if label == "Supported":
                        st.success(f"âœ… **CHÃNH XÃC**")
                    elif label == "Refuted":
                        st.error(f"âŒ **SAI Sá»° THáº¬T**")
                    else:
                        st.warning(f"â“ **KHÃ”NG Äá»¦ THÃ”NG TIN**")