import os
import json
import re
from typing import List, Dict, Any
import sys

from dotenv import load_dotenv
from google import genai

# ------------------------------------------------------------------
# Path handling to ensure 'src' imports work correctly
# ------------------------------------------------------------------
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, "../../../"))

sys.path.append(PROJECT_ROOT)
sys.path.append(os.path.join(PROJECT_ROOT, "src"))


# ======================================================================
# CONFIGURATION
# ======================================================================

# Load variables from .env file
load_dotenv()

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

if not GOOGLE_API_KEY:
    raise RuntimeError("GOOGLE_API_KEY is not set")

MODEL_NAME = "models/gemini-2.5-flash"  # Free tier, model t·ªìn t·∫°i th·∫≠t
BASE_INDEX = 1

# Initialize Gemini client
client = genai.Client(api_key=GOOGLE_API_KEY)

CONTEXT_GENERATION_PROMPT = """
B·∫°n l√† chuy√™n gia t·∫°o ng·ªØ c·∫£nh cho d·ªØ li·ªáu ki·ªÉm tra s·ª± th·∫≠t.

Ch·ªß ƒë·ªÅ: {topic}

Cho c√°c C√ÇU TUY√äN B·ªê:
{statements} v√† nh√£n c·ªßa ch√∫ng {labels}

H√£y t·∫°o ra M·ªòT ƒëo·∫°n CONTEXT GI·∫¢ sao cho:
- Ph√π h·ª£p v·ªõi ch·ªß ƒë·ªÅ
- Logic, t·ª± nhi√™n nh∆∞ b√†i b√°o
- Sao ch√©p nguy√™n vƒÉn statements nh∆∞ng b·ªï sung th√™m c√°c t·ª´ n·ªëi ƒë·ªÉ c√¢u vƒÉn tr√¥i ch·∫£y, m·∫°ch l·∫°c

CH·ªà TR·∫¢ V·ªÄ M·ªòT CHU·ªñI VƒÇN B·∫¢N (string).
KH√îNG JSON, KH√îNG markdown, KH√îNG gi·∫£i th√≠ch.
"""


# ======================================================================
# UTILITY FUNCTIONS
# ======================================================================

def generate_fake_context(
    topic: str,
    statements: List[str],
    labels: List[int],
) -> str:
    prompt = CONTEXT_GENERATION_PROMPT.format(
        topic=topic,
        statements="\n".join(f"- {s}" for s in statements),
        labels="\n".join(f"- {l}" for l in labels),
    )

    response = client.models.generate_content(
        model=MODEL_NAME,
        contents=prompt,
    )

    return response.text.strip()

def process_grouped_full_context(
    input_path: str,
    output_path: str,
    max_items: int = None,
):
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    results = []
    success = 0

    for idx, item in enumerate(data):
        if max_items and idx >= max_items:
            break

        try:
            # ----------------------------------------
            # Extract statements + labels from new format
            # ----------------------------------------
            statements_text = [s["text"] for s in item["statements"]]
            labels = [s["label"] for s in item["statements"]]

            fake_context = generate_fake_context(
                topic=item["topic"],
                statements=statements_text,
                labels=labels,
            )

            results.append({
                "topic": item["topic"],
                "Context": fake_context,
                # gi·ªØ nguy√™n structure ƒë·ªÉ d√πng v·ªÅ sau
                "Statement_list": [s["text"] for s in item["statements"]],
            })

            success += 1
            print(f"‚úÖ Generated context {success}")

        except Exception as e:
            print(f"‚ùå Failed at index {idx}: {e}")

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"\nüéâ Done: {success}/{len(data)} samples")


# ======================================================================
# ENTRY POINT
# ======================================================================

if __name__ == "__main__":
    input_file = os.path.join(
        PROJECT_ROOT,
        "experiments/synthesis_data_generation/dev/results/dev_parse_data/dev_grouped_full_context.json"
    )

    output_file = os.path.join(
        PROJECT_ROOT,
        "experiments/synthesis_data_generation/dev/results/dev_synthesis_data/dev_fake_context.json"
    )

    process_grouped_full_context(
        input_path=input_file,
        output_path=output_file,
        max_items=20,   # ƒë·ªÉ test tr∆∞·ªõc
    )
